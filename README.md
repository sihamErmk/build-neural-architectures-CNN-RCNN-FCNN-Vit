# **Lab 2: Deep Learning for Computer Vision with PyTorch** ğŸ–¥ï¸ğŸ”¬  

![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red?style=flat&logo=pytorch)  
![Status](https://img.shields.io/badge/Status-Completed-success)  
![License](https://img.shields.io/badge/License-MIT-blue)  

## **ğŸ“Œ Overview**  
Bienvenue dans **Lab 2: Deep Learning for Computer Vision**, un projet qui explore diffÃ©rentes architectures pour la classification dâ€™images avec le dataset **MNIST** ğŸ–¼ï¸. Ce projet fait partie du cours **Deep Learning** de lâ€™**UniversitÃ© Abdelmalek Essaadi**.  

### **ModÃ¨les dÃ©veloppÃ©s :**  
âœ… **CNN (Convolutional Neural Network)** : Un rÃ©seau de neurones convolutionnel classique pour la classification des chiffres manuscrits.  
âœ… **Faster R-CNN (Region-Based CNN)** : Un modÃ¨le avancÃ© basÃ© sur ResNet50 pour une classification plus robuste.  
âœ… **VGG16 & AlexNet** : Deux modÃ¨les prÃ©-entraÃ®nÃ©s affinÃ©s sur MNIST via *Transfer Learning*.  
âœ… **Vision Transformer (ViT)** : Un modÃ¨le *Transformer* entraÃ®nÃ© de zÃ©ro pour explorer une approche moderne en Vision par Ordinateur.  

Ce projet inclut l'**implÃ©mentation**, l'**entraÃ®nement**, l'**ajustement des hyperparamÃ¨tres**, et la **comparaison des performances** entre ces architectures. L'ensemble est exÃ©cutÃ© sur **Google Colab** avec accÃ©lÃ©ration **GPU**.  

---

## **ğŸ”§ Features**  
ğŸ“Œ **Classification MNIST** : ImplÃ©mente et compare 5 architectures (CNN, Faster R-CNN, VGG16, AlexNet, ViT).  
ğŸ“Œ **Analyse de Performance** : Ã‰valuation basÃ©e sur l'**accuracy**, le **F1-score**, la **perte (loss)** et le **temps d'entraÃ®nement**.  
ğŸ“Œ **GPU AccÃ©lÃ©rÃ©** : Utilisation d'un GPU **T4** pour des entraÃ®nements plus rapides.  
ğŸ“Œ **Fine-Tuning** : Adaptation de **VGG16 et AlexNet** Ã  MNIST (1 canal, 10 classes).  
ğŸ“Œ **Vision Transformer** : ImplÃ©mentation de ViT avec **patch embeddings** et **transformer layers**.  

---

## **ğŸ“Œ Table of Contents**  
- [Installation](#installation)  
- [Utilisation](#utilisation)  
- [Architecture du Projet](#architecture-du-projet)  
- [RÃ©sultats](#rÃ©sultats)  
- [Conclusion](#conclusion)  
- [Auteur](#auteur)  
- [License](#license)  

---

## **ğŸ› ï¸ Installation**  

### **ğŸ“Œ PrÃ©-requis**  
Avant de commencer, assure-toi d'avoir :  
âœ… **Python 3.8+**  
âœ… **Google Colab ou Jupyter Notebook**  
âœ… **Une machine avec GPU (optionnel, mais recommandÃ©)**  

### **ğŸ“¦ DÃ©pendances**  
Installe les bibliothÃ¨ques nÃ©cessaires avec :  
```bash
pip install torch torchvision numpy scikit-learn matplotlib tqdm
```
Si tu utilises un **environnement virtuel** :  
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### **ğŸ“‚ Cloner le projet**  
```bash
git clone https://github.com/votre-utilisateur/lab2-computer-vision.git
cd lab2-computer-vision
```

---

## **ğŸš€ Utilisation**  

1ï¸âƒ£ **Lancer le Notebook**  
ğŸ“œ Ouvre **`lab2_computer_vision.ipynb`** sur Jupyter ou **Google Colab**.  

2ï¸âƒ£ **ExÃ©cuter le code**  
Clique sur **"Run All"** ou exÃ©cute chaque cellule individuellement :  
- TÃ©lÃ©charge **MNIST** via `torchvision`.  
- EntraÃ®ne et Ã©value **tous les modÃ¨les**.  
- Affiche les rÃ©sultats sous forme de **graphes et mÃ©triques**.  

3ï¸âƒ£ **AccÃ©lÃ©ration GPU**  
Active le **GPU** sur Colab :  
ğŸ“Œ **Runtime > Change runtime type > GPU**  

---

## **ğŸ“‚ Architecture du Projet**  
```
lab2-computer-vision/
â”œâ”€â”€ lab2_computer_vision.ipynb  # Notebook principal avec implÃ©mentation
â”œâ”€â”€ requirements.txt            # Liste des dÃ©pendances
â”œâ”€â”€ results/                    # Dossier contenant les rÃ©sultats et graphiques
â””â”€â”€ README.md                   # Ce fichier ğŸ“„
```

Le dataset **MNIST** est tÃ©lÃ©chargÃ© dynamiquement via `torchvision.datasets.MNIST`.

---

## **ğŸ“Š RÃ©sultats**  

ğŸ“Œ **Comparaison des ModÃ¨les** (aprÃ¨s exÃ©cution)  

| ModÃ¨le       | Accuracy | F1-Score | Loss  | Training Time |
|-------------|----------|---------|------|--------------|
| **CNN**     | [TBD] % | [TBD]   | [TBD] | [TBD] s |
| **Faster R-CNN** | [TBD] % | [TBD] | [TBD] | [TBD] s |
| **VGG16**   | [TBD] % | [TBD]   | [TBD] | [TBD] s |
| **AlexNet** | [TBD] % | [TBD]   | [TBD] | [TBD] s |
| **ViT**     | [TBD] % | [TBD]   | [TBD] | [TBD] s |

---

## **ğŸ“Œ Conclusion**  
ğŸ’¡ *Les rÃ©seaux convolutionnels classiques restent dominants pour MNIST, mais ViT montre un potentiel intÃ©ressant pour des datasets plus complexes.*  

---

## **ğŸ‘¨â€ğŸ’» Auteur**  

| Nom | GitHub |
|----|--------|
| ğŸ“ **[ sihamErmk]** | [@sihamErmk](https://github.com/sihamErmk) |

---

## **ğŸ“œ License**  
ğŸ”– Ce projet est sous licence **MIT** â€“ Utilisation libre, mais sans garantie.  

---

## **ğŸ™Œ Remerciements**  
ğŸ”¥ **PyTorch Team** â€“ Pour une librairie incroyable.  
ğŸ’» **Google Colab** â€“ Pour un accÃ¨s GPU gratuit.  
ğŸ“Š **MNIST Dataset** â€“ Pour Ãªtre un benchmark essentiel en Deep Learning.  

---

ğŸš€ **Bon apprentissage et bon code !** ğŸ¯
